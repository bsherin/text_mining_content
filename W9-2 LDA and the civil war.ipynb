{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "Here's some of my functions that print things nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "def check_float(potential_float):\n",
    "    try:\n",
    "        float(potential_float)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def round_if_float(v, prec=3):\n",
    "    if check_float(v):\n",
    "        return round(float(v), prec)\n",
    "    return v\n",
    "    \n",
    "from IPython.core.display import display, HTML\n",
    "def list_table(the_list, color_nums=False):\n",
    "    html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "    for row in the_list:\n",
    "        html.append(\"<tr>\")\n",
    "        for col in row:\n",
    "            if color_nums and check_float(col) and not float(col) == 0:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray; color: {1}; font-weight: bold'>{0}</td>\".format(round_if_float(col), color_nums))\n",
    "            else:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(round_if_float(col)))\n",
    "        html.append(\"</tr>\")\n",
    "    html.append(\"</table>\")\n",
    "    return display(HTML(''.join(html)))\n",
    "    \n",
    "import numpy\n",
    "class MultiTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = []\n",
    "        for l in self:\n",
    "            html.append(\"<table style= 'border: 1px solid black; display:inline-block; margin-right: 10px;'>\")\n",
    "            for row in l:\n",
    "                html.append(\"<tr>\")\n",
    "                for col in row:\n",
    "                    if isinstance(col, numpy.float32):\n",
    "                        col = str(round(col, 3))\n",
    "                    html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "                html.append(\"</tr>\")\n",
    "            html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyLDAvis  # Only execute this once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of the usual imports\n",
    "\n",
    "The big new libraries here are `gensim` and `pyLDAvis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the api to get some articles from wikipedia about the American civil war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"American Civil War\",\n",
    "    \"Abraham Lincoln\",\n",
    "    \"Slavery in the United States\",\n",
    "    \"Slave states and free states\",\n",
    "    \"Emancipation Proclamation\",\n",
    "    \"Robert E. Lee\",\n",
    "    \"Ulysses S. Grant\",\n",
    "    \"Conclusion of the American Civil War\",\n",
    "    \"Origins of the American Civil War\",\n",
    "    \"Issues of the American Civil War\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def underscorize(pagename):\n",
    "    return re.sub(\" \", \"_\", pagename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/Wikipedia-API/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the articles and put them in a dictionary.\n",
    "\n",
    "We'll split the articles up by paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dict = {}\n",
    "for page in pages:\n",
    "    pagename = underscorize(page)\n",
    "    print(pagename)\n",
    "    p_wiki = wiki_wiki.page(pagename)\n",
    "    page_text = p_wiki.text.split(\"\\n\")\n",
    "    page_paras = [para for para in page_text if len(para) > 1]\n",
    "    page_dict[pagename] = page_paras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordify each paragraph of each article of the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are keeping one big list of all of the wordified paragraphs, as well as storing the wordified paragraphs for each article in a dictionary.\n",
    "\n",
    "We'll use one of my highly tinkered with wordifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bruces_alpha_only(text, stem_text=False, alpha_only=True, lower_case=True):\n",
    "    import re\n",
    "    from nltk import stem  # @UnresolvedImport\n",
    "    \n",
    "    def is_contraction(the_text):\n",
    "        contraction_patterns = re.compile(r\"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\\b\")\n",
    "        return contraction_patterns.search(the_text)\n",
    "\n",
    "    def return_alpha_only (ltext):\n",
    "        return [w for w in ltext if (len(w) > 0) and (w.isalpha() or w[0]=='<' or is_contraction(w))]\n",
    "    \n",
    "    stemmer = stem.PorterStemmer()\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    punctuation_class = r\"([\\.\\-\\/&\\\";:\\(\\)\\?\\!\\]\\[\\{\\}\\*#])\"\n",
    "    \n",
    "    # Separate most punctuation at end of words\n",
    "\n",
    "    text = re.sub(r\"(\\w)\" + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Separate most punctuation at start of words\n",
    "    text = re.sub(punctuation_class + r\"(\\w)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate punctuation from other punctuation\n",
    "    text = re.sub(punctuation_class + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Put spaces between + and = signs and digits. Also %s that follow a digit, $s that come before a digit\n",
    "    text = re.sub(r\"(\\d)([+=%])\", r'\\1 \\2 ', text)\n",
    "    text = re.sub(r\"([\\$+=])(\\d)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate commas if they're followed by space.\n",
    "    # (E.g., don't separate 2,500)\n",
    "    text = re.sub(r\"(,\\s)\", r' \\1', text)\n",
    "    \n",
    "    #when we have two double quotes make it 1.\n",
    "    #\n",
    "    text = re.sub(\"\\\"\\\"\", \"\\\"\", text)\n",
    "\n",
    "    # Separate leading and trailing single and double quotes .\n",
    "    text = re.sub(r\"(\\'\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(\\\"\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"('\\'$)\", r' \\1', text)\n",
    "    text = re.sub(r\"('\\\"$)\", r' \\1', text)\n",
    "\n",
    "    #Separate parentheses where appropriate\n",
    "    text = re.sub(r\"(\\)\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\()\", r'\\1 ', text)\n",
    "\n",
    "    # Separate periods that come before newline or end of string.\n",
    "    text = re.sub('\\. *(\\n|$)', ' . ', text)\n",
    "    \n",
    "    # separate single quotes in the middle of words\n",
    "    # text = re.sub(r\"(\\w)(\\')(\\w)\", r'\\1 \\2 \\3', text)\n",
    "    \n",
    "    # separate out 's at the end of words\n",
    "    text = re.sub(r\"(\\w)(\\'s)(\\s)\", r\"\\1 s \", text)\n",
    "    split_text = text.split()\n",
    "    \n",
    "    if stem_text:\n",
    "        result = [stemmer.stem(w) for w in split_text]\n",
    "        split_text = result\n",
    "\n",
    "    if alpha_only:\n",
    "        split_text = return_alpha_only(split_text)\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordified_paras = []\n",
    "wordified_page_dict = {}\n",
    "for name, page in page_dict.items():\n",
    "    wordified_page_paras = []\n",
    "    for para in page:\n",
    "        wordified_page_paras.append(bruces_alpha_only(para))\n",
    "    wordified_page_dict[name] = wordified_page_paras\n",
    "    wordified_paras += wordified_page_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordified_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(wordified_paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/stop-words_english_1_en.txt\")\n",
    "stop_list = f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_no_stop = []\n",
    "for doc in wordified_paras:\n",
    "    new_doc = [w for w in doc if w not in stop_list]\n",
    "    docs_no_stop.append(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the corpus in the form that Gensim linkes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gensim dictionary maps every token (i.e, word) to a number. It also computes some frequencies for us.\n",
    "\n",
    "https://radimrehurek.com/gensim/corpora/dictionary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict = corpora.Dictionary(docs_no_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict.token2id[\"lincoln\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict.dfs[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_no_stop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gensim dictionary can be used to create a vector from a document.\n",
    "\n",
    "Gensim does a better job of representing sparse vectors - vectors with lots of zeros in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gensim_dict.doc2bow(docs_no_stop[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all of the documents to bags of words, in the gensim manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_bows = [gensim_dict.doc2bow(doc) for doc in docs_no_stop] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to pass our bags of words and gensim dictionary to gensim.\n",
    "\n",
    "But there are many, many parameters to specify. And many different ways we might pre-process. And they make a difference.\n",
    "\n",
    "Also, this won't give the same result every time, unless we specify `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m1 = models.LdaModel(cw_bows, \n",
    "                         id2word=gensim_dict, \n",
    "                         num_topics=8,\n",
    "                         passes=10,\n",
    "                         update_every=0,\n",
    "                          chunksize=2000,\n",
    "                          iterations=100,\n",
    "                          gamma_threshold=.001,\n",
    "                          decay=.5,\n",
    "                          offset=1,\n",
    "                          random_state=1,\n",
    "                          alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m1.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a couple of functions that print the results a bit more nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_from_topic(lda_model, gdict, n, to_print=10, min_oc=10):\n",
    "    result = [[\"word\", \"weight\"]]\n",
    "    topic_words = lda_model.show_topic(n, 100)\n",
    "    cnt = 0\n",
    "    i = 0 \n",
    "    while cnt < to_print:\n",
    "        res = topic_words[i]\n",
    "        w = res[0]\n",
    "        df = gdict.dfs[gdict.token2id[w]]\n",
    "        if df > min_oc:\n",
    "            result.append([res[0], round(res[1], 3)])\n",
    "            cnt += 1\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "def top_words_from_topics(lda_model, gdict, to_print=10, min_oc=10):\n",
    "    result = []\n",
    "    for n in range(lda_model.num_topics):\n",
    "        result.append(top_words_from_topic(lda_model, gdict, n, to_print, min_oc))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_topics(lda_m1, gensim_dict, min_oc=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with some different settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the `number of topics` to 5.\n",
    "\n",
    "Also let's `filter extremes` (words that appear too much or too little).\n",
    "\n",
    "And, let's up the number of `passes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict.filter_extremes(no_below=5, no_above=0.25) \n",
    "cw_bows = [gensim_dict.doc2bow(doc) for doc in docs_no_stop] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m2 = models.LdaModel(cw_bows, \n",
    "                         id2word=gensim_dict, \n",
    "                         num_topics=5,\n",
    "                          passes=50,\n",
    "                          update_every=0,\n",
    "                          chunksize=2000,\n",
    "                          iterations=100,\n",
    "                          gamma_threshold=.001,\n",
    "                          decay=.5,\n",
    "                          offset=1,\n",
    "                          random_state=2,\n",
    "                          alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_topics(lda_m2, gensim_dict, min_oc=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis\n",
    "pyLDAvis is a library that draws some useful diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "vis_data = gensimvis.prepare(lda_m2, cw_bows, gensim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the topics in individual documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First gather all of the info we'll need\n",
    "\n",
    "* each wordified page\n",
    "* bow for each page\n",
    "* topics for each overall page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagenames = list(wordified_page_dict.keys())\n",
    "wordified_pages = {}\n",
    "article_bows = {}\n",
    "article_topics = {}\n",
    "for pagename in pagenames:\n",
    "    wordified_page_rows = wordified_page_dict[pagename]\n",
    "    flat_list = []\n",
    "    for r in wordified_page_rows:\n",
    "        for item in r:\n",
    "            flat_list.append(item)\n",
    "    wordified_pages[pagename] = flat_list\n",
    "    article_bows[pagename] = gensim_dict.doc2bow(flat_list)\n",
    "    article_topics[pagename] = lda_m2[article_bows[pagename]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_topics[pagename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the topics for the pages in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tarray = np.zeros([len(pagenames), lda_m2.num_topics])\n",
    "for n, pagename in enumerate(pagenames):\n",
    "    art_topics = article_topics[pagename]\n",
    "    for t in art_topics:\n",
    "        tarray[n, t[0]] = t[1]\n",
    "tarray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a heatmap for the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = []\n",
    "for topic in top_words_from_topics(lda_m2, gensim_dict, min_oc=1):\n",
    "    label = topic[1][0] + \"-\" + topic[2][0]\n",
    "    topic_labels.append(label)\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "fig = matplotlib.pyplot.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "dialogs = []\n",
    "(nrows, ncols) = tarray.shape\n",
    "cax = ax.imshow(tarray, cmap=cm.gist_yarg, aspect=\"auto\", interpolation='nearest')\n",
    "\n",
    "ind = np.arange(ncols)\n",
    "ax.set_xticks(ind, minor=False)\n",
    "ax.set_xticks(ind + .5, minor=True)\n",
    "ax.get_xaxis().set_ticklabels(topic_labels, size=\"medium\", rotation=\"vertical\")\n",
    "\n",
    "ind = np.arange(nrows)\n",
    "\n",
    "ax.set_yticks(ind, minor=False)\n",
    "ax.set_yticks(ind + .5, minor=True)\n",
    "ax.get_yaxis().set_ticklabels(pagenames, size=\"small\", rotation=\"horizontal\")\n",
    "\n",
    "ax.grid(True, which='minor', linestyle=':')\n",
    "\n",
    "fig.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the topics in an individual article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordified_page = wordified_page_dict[\"American_Civil_War\"]\n",
    "wordified_page = wordified_page_dict[\"Emancipation_Proclamation\"]\n",
    "article_bows = [gensim_dict.doc2bow(para) for para in wordified_page]\n",
    "article_topics = lda_m2[article_bows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tarray = np.zeros([lda_m2.num_topics, len(article_topics)])\n",
    "for n, para in enumerate(article_topics):\n",
    "    for t in para:\n",
    "        tarray[t[0], n] = t[1]\n",
    "tarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "fig = matplotlib.pyplot.figure(figsize=(25, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "dialogs = []\n",
    "nrows, ncols = tarray.shape\n",
    "cax = ax.imshow(tarray, cmap=cm.gist_yarg, aspect=\"auto\", interpolation='nearest')\n",
    "indices = np.arange(ncols)\n",
    "ax.set_xticks(indices, minor=False)\n",
    "ax.set_xticks(indices + .5, minor=True)\n",
    "ax.get_xaxis().set_ticklabels(list(range(ncols)), size=\"medium\", rotation=\"vertical\")\n",
    "indices = np.arange(nrows)\n",
    "ax.set_yticks(indices, minor=False)\n",
    "ax.set_yticks(indices + .5, minor=True)\n",
    "ax.get_yaxis().set_ticklabels(topic_labels, size=\"small\", rotation=\"horizontal\")\n",
    "\n",
    "ax.grid(True, which='minor', linestyle=':')\n",
    "\n",
    "fig.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top paragraphs for each topic\n",
    "\n",
    "First we build up a data structure. This will include the topic weights for each paragraph, as well as the text of each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_topics = []\n",
    "for pagename, wordified_page in wordified_page_dict.items():\n",
    "    print(pagename)\n",
    "    wordified_page = wordified_page_dict[pagename]\n",
    "    article_bows = [gensim_dict.doc2bow(para) for para in wordified_page]\n",
    "    article_topics = lda_m2[article_bows]\n",
    "    page_data = []\n",
    "    for i, topics in enumerate(article_topics):\n",
    "        para_topics.append([topics, pagename, i, wordified_page[i], page_dict[pagename][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions sort this big data structure by the weights of one of the topics and then print the top results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_topic_weight(tlist, topicnum):\n",
    "    for tup in tlist:\n",
    "        if tup[0] == topicnum:\n",
    "            return tup[1]\n",
    "        return -1\n",
    "\n",
    "def sort_by_topic(ptopics, topicnum):\n",
    "    def sfunc(item):\n",
    "        return get_topic_weight(item[0], topicnum)\n",
    "    nptopics = copy.deepcopy(ptopics)\n",
    "    nptopics.sort(key=sfunc, reverse=True)\n",
    "    return nptopics\n",
    "\n",
    "def print_top_for_topic(ptopics, topicnum, to_print=10):\n",
    "    sorted_paras = sort_by_topic(para_topics, topicnum)\n",
    "    result_table = [[\"page\", \"text\", \"weight\"]]\n",
    "    for presult in sorted_paras[:to_print]:\n",
    "        result_table.append([presult[1], presult[4], round(get_topic_weight(presult[0], topicnum), 3)])\n",
    "    return ListTable(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_top_for_topic(para_topics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
