{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasons Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the corpus and get the vocabulary, as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus\n",
    "seasons_corpus = load_seasons_corpus()\n",
    "fnames = list(seasons_corpus.keys())\n",
    "docs = [seasons_corpus[fname][0] for fname in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for doc in docs:\n",
    "    set_vocab = set_vocab.union(set(doc))\n",
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))\n",
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for word in pruned_vocab:\n",
    "    word_fdist[word] = 0\n",
    "    doc_fdist[word] = 0\n",
    "    for doc in docs:\n",
    "        if word in doc:\n",
    "            doc_fdist[word] += 1\n",
    "            word_fdist[word] += doc.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [w[0] for w in word_fdist.most_common(500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(vec):\n",
    "    mag = np.dot(vec, vec)\n",
    "    if mag == 0:\n",
    "        return vec\n",
    "    else:\n",
    "        return(vec / np.sqrt(mag))\n",
    "    \n",
    "def pure_tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def tf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0 or df == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) / df\n",
    "    return result\n",
    "\n",
    "def weight_factor2(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def weighted_word(the_text, word):\n",
    "    return tf(the_text.count(word), doc_fdist[word], word_fdist[word], len(seasons_corpus.keys()))\n",
    "\n",
    "def compute_doc_vector(word_list):\n",
    "    return norm_vec([weighted_word(word_list, word) for word in vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    doc_vectors.append(compute_doc_vector(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put vectors in a matrix to make sklean happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=3).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "def get_centroids(X, labels):\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, labels)\n",
    "    centroids = clf.centroids_\n",
    "    return centroids\n",
    "\n",
    "centroids = get_centroids(X, clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_from_centroid(centroids, n, to_print=10, printit=True):\n",
    "    sc = list(np.argsort(centroids[n]))\n",
    "    sc.reverse()\n",
    "    result = []\n",
    "    for i in range(to_print):\n",
    "        if printit:\n",
    "            print(vocab_list[sc[i]], round(centroids[n][sc[i]], 3))\n",
    "        result.append([vocab_list[sc[i]], round(centroids[n][sc[i]], 3)])\n",
    "    return result\n",
    "\n",
    "def top_words_from_centroids(centroids, to_print=10, printit=False):\n",
    "    result = []\n",
    "    for n in range(len(centroids)):\n",
    "        if printit:\n",
    "            print(\"Cluster \", n)\n",
    "            top_words_from_centroid(centroids, n, to_print, printit=printit)\n",
    "            print(\"\\n\")\n",
    "        result.append(top_words_from_centroid(centroids, n, to_print, printit=printit))\n",
    "    return result\n",
    "        \n",
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "class MultiTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = []\n",
    "        for l in self:\n",
    "            html.append(\"<table style= 'border: 1px solid black; display:inline-block; margin-right: 10px;'>\")\n",
    "            for row in l:\n",
    "                html.append(\"<tr>\")\n",
    "                for col in row:\n",
    "                    html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "\n",
    "                html.append(\"</tr>\")\n",
    "            html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for n, fname in enumerate(fnames):\n",
    "    res.append([fname, clustering.labels_[n]])\n",
    "ListTable(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rss(X, centroids, labels):\n",
    "    rss = 0\n",
    "    for dnum in range(X.shape[0]):\n",
    "        vec = X[dnum]\n",
    "        centroid = centroids[labels[dnum]]\n",
    "        res_vec = vec - centroid\n",
    "        rss += np.dot(res_vec, res_vec)\n",
    "    return rss\n",
    "\n",
    "def compute_rss_data_agglomerative(X, start, end):\n",
    "    results = []\n",
    "    for k in range(start, end):\n",
    "        clusterer = AgglomerativeClustering(n_clusters=k, linkage=\"ward\", affinity=\"euclidean\")\n",
    "        clusterer.fit(X)\n",
    "        labels = clusterer.labels_\n",
    "        centroids = get_centroids(X, labels)\n",
    "        rss = compute_rss(X, centroids, labels)\n",
    "        results.append([k, rss])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_list = compute_rss_data_agglomerative(X, 2, 10)\n",
    "plt.clf()\n",
    "x = [res[0] for res in rss_list]\n",
    "y = [res[1] for res in rss_list]\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer \n",
    "from sklearn.cluster import KMeans\n",
    "clusterer = KMeans()\n",
    "visualizer = KElbowVisualizer(clusterer, k=(2, 10), metric=\"silhouette\")\n",
    "visualizer.fit(X)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
